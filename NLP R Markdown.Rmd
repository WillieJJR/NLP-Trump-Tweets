---
title: "NLP - Twitter"
author: "Willie Cheeks"
date: "12/7/2020"
output: html_document
---
Data Source: https://www.kaggle.com/gpreda/trump-tweets

The goal of this project is to analyze a subset of President Trump's tweets over the course of 2020 (July-November) to identify 3 things:
-The overarching topics of his tweets (Topic Modeling)
-The emotional tone of his tweets (Opinion Mining)
-Patterns of people & places mentioned (Entity Recognition)

```{r}
#Collect data 
trumpTweets <- read.csv("trump_tweets.csv")
head(trumpTweets, 10)
```

Part 1: Topic Modeling - I want to better understand the over-arching Topics of President Trumps tweets by using just a few words he frequently uses 

```{r cars}
#Required Packages
install.packages("tidyverse")
install.packages("tm")
install.packages("tidytext")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("wordcloud") 
install.packages("pals")
library(tidyverse)
library(tm)
library(tidytext)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)

#extracting text data 
trumpTweets <- as.data.frame(trumpTweets[,11])
trumpTweets <- trumpTweets %>% 
  rename(tweets = `trumpTweets[, 11]`)

#Create Corpus for text mining 
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
corpusTweets <- Corpus(VectorSource(trumpTweets$tweets))

#text processing 
add_space <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) #functions to add spaces before special punctuation 
corpusTweets <- tm_map(corpusTweets,
                       add_space, "-")
corpusTweets <- tm_map(corpusTweets,
                       add_space, ":")
corpusTweets <- tm_map(corpusTweets,
                       add_space, "'")
corpusTweets <- tm_map(corpusTweets,
                       add_space, " -")
corpusTweets <- tm_map(corpusTweets,
                       add_space, "~")

processedTweets <- tm_map(corpusTweets,
                          removePunctuation, preserve_intra_word_dashes = TRUE) #remove punctuation
processedTweets <- tm_map(processedTweets,
                          removeWords, c(stopwords("english"), "https", "...", "the", "you", "for", "get")) #remove english stopwords
processedTweets <- tm_map(processedTweets,
                          removeNumbers) #remove numbers
processedTweets <- tm_map(processedTweets,
                          content_transformer(tolower)) #standardize data by remove capital letters
processedTweets <- tm_map(processedTweets,
                          stemDocument, language = "en") #stemming
#test <- tm_map(processedTweets, stripWhitespace)

```


```{r pressure, echo=FALSE}
#Model Development 
minimumFrequency <- 7 #used as baseline for dtm - using higher to have manageable/more accurate outputs
dtm <- DocumentTermMatrix(processedTweets, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(dtm) #check dimensions
rowTotals <- apply(dtm , 1, sum) #Find the sum of words per row
dtm.new   <- dtm[rowTotals> 0, ] #remove rows w/o words

K <- 5 #number of topics to discover
set.seed(10) #maintain reproducible results
topicModel <- LDA(dtm.new, K, method="Gibbs", control=list(iter = 500, verbose = 25)) #compute LDA model
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r}
nTerms(dtm.new) #396 terms 
terms(topicModel, 20) #preview of clusters
```

```{r}
#obtain beta (probability distribution) for each topic 
trump_topics <- tidy(topicModel, matrix = "beta")

 
trumpTop_terms <- trump_topics %>%
  group_by(topic) %>%
  top_n(50, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
#develop word cloud for probability distribution of words within each topic 
trumpTop_terms %>%
  mutate(topic = paste("Topic", topic, sep = " : ")) %>%
  acast(term ~ topic, value.var = "beta", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4", "#00FF00", "#FFFF00", "#FFA500" ),
                   max.words = 60)
```

Part 2: Opinion Mining - I want to better understand the tone of the words President Trump has used in his tweets

```{r}
#Load in packages
install.packages("plotly")
install.packages("textdata")
library(textdata)
library(plotly)

#extracting text data 
trumpTweets <- as.data.frame(trumpTweets[,11])
trumpTweets <- trumpTweets %>% 
  rename(tweets = `trumpTweets[, 11]`)

#cleaning data
trumpTweets_clean <- mutate(trumpTweets, tweets = as.character(trumpTweets$tweets))
trump_tweets_clean <- trumpTweets_clean %>%
  select(tweets) %>%
  unnest_tokens(word, tweets)

head(trump_tweets_clean)

```

```{r}
#using in place of sentiment function 
classify_emotion <- function(textColumns,algorithm="bayes",prior=1.0,verbose=FALSE,...) {
	matrix <- create_matrix(textColumns,...)
	lexicon <- read.csv(system.file("data/emotions.csv.gz",package="sentiment"),header=FALSE)

counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
	documents <- c()

	for (i in 1:nrow(matrix)) {
		if (verbose) print(paste("DOCUMENT",i))
		scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
		doc <- matrix[i,]
		words <- findFreqTerms(doc,lowfreq=1)
		
		for (word in words) {
            for (key in names(scores)) {
                emotions <- lexicon[which(lexicon[,2]==key),]
                index <- pmatch(word,emotions[,1],nomatch=0)
                if (index > 0) {
                    entry <- emotions[index,]
                    
                    category <- as.character(entry[[2]])
                    count <- counts[[category]]
        
                    score <- 1.0
                    if (algorithm=="bayes") score <- abs(log(score*prior/count))
            
                    if (verbose) {
                        print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
                    }
                    
                    scores[[category]] <- scores[[category]]+score
                }
            }
        }
        
        if (algorithm=="bayes") {
            for (key in names(scores)) {
                count <- counts[[key]]
                total <- counts[["total"]]
                score <- abs(log(count/total))
                scores[[key]] <- scores[[key]]+score
            }
        } else {
            for (key in names(scores)) {
                scores[[key]] <- scores[[key]]+0.000001
            }
        }
		
        best_fit <- names(scores)[which.max(unlist(scores))]
        if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
		documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
	}
	
	colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
	return(documents)
}
```



```{r}
#classify sentiments
install.packages("textdata")
library(textdata)

#using NRC lexicon 
#text_emotion <- get_sentiments("bing")
text_emotion <- get_sentiments("nrc")

#match emotions to trump tweets
trumpSentiment <- trump_tweets_clean %>%
  inner_join(text_emotion)

trumpSentiment
```


```{r}
#perform initial sentiment analysis 

trumpSentiment_count <- trumpSentiment %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
trumpSentiment_count

trumpSentiment_count$sentiment <- factor(trumpSentiment_count$sentiment, levels = unique(trumpSentiment_count$sentiment)[order(trumpSentiment_count$n, decreasing = TRUE)])

plot_ly(data = trumpSentiment_count, 
            x = ~sentiment, 
            y = ~n, 
            color = ~sentiment,
            #marker = list(color =pal), 
            type = "bar") %>%
    layout(xaxis=list(title=""), showlegend=F,
         title="Emotion Mining for President Trumps Tweets (07/2020 - 11/2020)",
         yaxis = list(title = "Number of Tweets"))

```

Part 3: Entity Recognition - I want to understand the most important nouns President Trump referenced to better understand what his priorities may have been at the time of these tweets 

```{r}
#Load in packages
install.packages("NLP")
install.packages("openNLP")
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at")
install.packages("rJava")
install.packages("treemapify")
install.packages("gghighlight")
library(NLP)
library(openNLP)
library(rJava)
library(plotly)
library(treemapify)
library(gghighlight)


#extracting text data
trumpTweets <- read.csv("trump_tweets.csv")

trumpTweets <- trumpTweets[trumpTweets$favorites > 10000,] #used to filter data to work with annotate()

trumpTweets <- as.data.frame(trumpTweets[,11])
trumpTweets <- trumpTweets %>% 
  rename(tweets = `trumpTweets[, 11]`)

trumpTweets_vec <- paste(unlist(trumpTweets), collapse =" ") #have to change the df into a character vector to have all text in one row

trumpTweets_vec <- as.String(trumpTweets_vec) #openNLP can only work with string vectors 

```

```{r}
#create annotation object

word_ann <- Maxent_Word_Token_Annotator() #using for word annotations
sent_ann <- Maxent_Sent_Token_Annotator() #using for sentence annotations

tweet_annotations <- NLP::annotate(trumpTweets_vec, list(sent_ann, word_ann)) #creating an annotation object

tweet_ptd <- AnnotatedPlainTextDocument(trumpTweets_vec, tweet_annotations)
tweet_ptd

head(sents(tweet_ptd), 5)
head(words(tweet_ptd), 10)
```

```{r}
#annotating people and places

person_ann <- Maxent_Entity_Annotator(kind = "person") #function to recognize people
location_ann <- Maxent_Entity_Annotator(kind = "location") #function to recognize places
organization_ann <- Maxent_Entity_Annotator(kind = "organization") #functions to recognize orgs


pipeline <- list(sent_ann,
                 word_ann,
                 person_ann,
                 location_ann,
                 organization_ann) #create a pipeline to hold annotators
trumpTweets_annotations <- NLP::annotate(trumpTweets_vec, pipeline) #create annotation object
trumpTweets_ptd <- AnnotatedPlainTextDocument(trumpTweets_vec, trumpTweets_annotations)

```

```{r}
#function for entity recognition output

entities <- function(doc, kind) {
         s <- doc$content
         a <- annotation(doc)
         if(hasArg(kind)) {
             k <- sapply(a$features, `[[`, "kind")
             s[a[k == kind]]
         } else {
             s[a[a$type == "entity"]]
         }
    }

```
```{r}
#quantify the output for most popular people mentioned by frequency
person <- entities(trumpTweets_ptd, kind = 'person')

freq_person <- as.data.frame(table(person))
freq_person
freq_person %>% 
    arrange(desc(Freq))

#graph output - Top 10 people mentioned by the President 
person_Top_10 <- freq_person %>% 
  arrange(desc(freq_person$Freq)) %>%
  slice(1:10)

bar_people <- ggplot(person_Top_10, aes(x=reorder(person, -Freq), y=Freq, name = person)) + 
  geom_point(size=5, color = "red", fill=alpha("blue", 0.3), alpha=0.7, shape=21, stroke=2) + 
  geom_segment(aes(x=reorder(person, -Freq), 
                   xend=reorder(person, -Freq), 
                   y=0, 
                   yend=Freq)) + 
  labs(title="Who's on the President's Mind?", 
       subtitle="Top 5 People Mentioned in Ex-President Trumps Tweets", 
       caption="Source: Twitter",
       x = "People",
       y = "Frequency of Mentions") +
  theme_classic()

bar_people <- bar_people + theme(axis.text.x = element_text(angle = 45))
ggplotly(bar_people, tooltip = c("name","Freq"))


```
```{r}
#quantify the output for most popular locations mentioned by frequency
#graph output - Top 10 places mentioned by the President 
loc <- entities(trumpTweets_ptd, kind = 'location')
freq_loc <- as.data.frame(table(loc))
freq_loc
freq_loc %>% 
    arrange(desc(Freq))

Loc_Top_10 <- freq_loc %>% 
  arrange(desc(freq_loc$Freq)) %>%
  slice(1:10)

Loc_Top_10

Loc_Tree <-ggplot(Loc_Top_10, aes(area = Freq, label = paste(loc, Freq, sep = "\n"), fill = as.factor(Freq))) +
   geom_treemap() +
   geom_treemap_text(
     colour = "black",
     place = "centre",
     size = 15,
   ) +
   scale_fill_brewer(palette = "Blues", name = "# of Mentions",
                     direction = 1,
                     )

Loc_Tree + theme(legend.position = "none")  


```

```{r}
#results for orgs
org <- entities(trumpTweets_ptd, kind = 'organization')

freq_org <- as.data.frame(table(org))
freq_org
freq_org %>% 
    arrange(desc(Freq))

#quantify the output for most popular orgs mentioned by frequency
#graph output - Top 10 porgs mentioned by the President 
Org_Top_10 <- freq_org %>% 
  arrange(desc(freq_org$Freq)) %>%
  slice(1:10)

Org_Top_10

#highlight max value 
Org_Top_10 <- Org_Top_10 %>% mutate( highlightMax = ifelse( Freq == max(Freq), "yes", "no" ) )

org_plot <- ggplot( Org_Top_10, aes( x = reorder(org, Freq), y = Freq, fill = highlightMax ) ) +
    geom_bar( stat = "identity", width = 1 ) +
    scale_fill_manual( values = c( "yes"="red", "no"="#0099f9" ), guide = FALSE )+
   labs(
     title="What's on the President's Mind?", 
      subtitle="Top 5 Organizations Mentioned in Ex-President Trumps Tweets", 
      caption="Source: Twitter",
     x = "Organizations",
     y = "Frequency of Mentions"
   )+
   coord_flip()+
   theme_classic()+
  theme(legend.position = "none")


ggplotly(org_plot, tooltip = c("Organization Name","Freq"))

```

